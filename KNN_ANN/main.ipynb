{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import wandb\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_embeddings = torch.load(\"./Dataset/train_embeddings.pth\")\n",
    "    train_labels = torch.load(\"./Dataset/train_labels.pth\")\n",
    "    test_embeddings = torch.load(\"./Dataset/test_embeddings.pth\")\n",
    "    test_labels = torch.load(\"./Dataset/test_labels.pth\")\n",
    "    text_embeddings = torch.load(\"./Dataset/text_embedding.pth\")\n",
    "    return train_embeddings, train_labels, test_embeddings, test_labels, text_embeddings\n",
    "\n",
    "# Visualization using t-SNE\n",
    "def visualize_tsne(embeddings, labels, title):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter, ticks=range(10), label='Classes')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "train_embeddings, train_labels, test_embeddings, test_labels, text_embeddings = load_data()\n",
    "\n",
    "train_embeddings_cpu = train_embeddings.cpu().numpy()\n",
    "train_labels_cpu = train_labels.cpu().numpy()\n",
    "test_embeddings_cpu = test_embeddings.cpu().numpy()\n",
    "test_labels_cpu = test_labels.cpu().numpy()\n",
    "text_embeddings_cpu = text_embeddings.cpu().numpy()\n",
    "\n",
    "visualize_tsne(train_embeddings_cpu, train_labels_cpu, \"t-SNE Visualization of Training Embeddings\")\n",
    "visualize_tsne(test_embeddings_cpu, test_labels_cpu, \"t-SNE Visualization of Test Embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task-1.1 Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return cdist(x1, x2.reshape(1, -1), metric='euclidean').flatten()\n",
    "\n",
    "def cosine_distance(x1, x2):\n",
    "    return cdist(x1, x2.reshape(1, -1), metric='cosine').flatten()\n",
    "\n",
    "def knn(train_embeddings, train_labels, query, k, metric=\"euclidean\", return_comparisons=False):\n",
    "    if metric == \"euclidean\":\n",
    "        distances = euclidean_distance(train_embeddings, query)\n",
    "    elif metric == \"cosine\":\n",
    "        distances = cosine_distance(train_embeddings, query)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric. Use 'euclidean' or 'cosine'.\")\n",
    "    \n",
    "    comparisons = len(train_embeddings)\n",
    "    \n",
    "    nearest_neighbors = np.argsort(distances)[:k]\n",
    "    \n",
    "    if return_comparisons:\n",
    "        return nearest_neighbors, comparisons\n",
    "    else:\n",
    "        return nearest_neighbors\n",
    "\n",
    "def classify(train_embeddings, train_labels, test_embeddings, test_labels, k, metric=\"euclidean\"):\n",
    "    correct = 0\n",
    "    total = len(test_labels)\n",
    "    for i in range(total):\n",
    "        knn_indices = knn(train_embeddings, train_labels, test_embeddings[i], k, metric)\n",
    "        predicted_label = np.bincount(train_labels[knn_indices]).argmax()\n",
    "        correct += (predicted_label == test_labels[i])\n",
    "    return correct / total\n",
    "\n",
    "# Evaluate KNN for different k values and distance metrics\n",
    "k_values = [1, 5, 10]\n",
    "distance_metrics = [\"euclidean\", \"cosine\"]\n",
    "\n",
    "for k in k_values:\n",
    "    for metric in distance_metrics:\n",
    "        accuracy = classify(train_embeddings_cpu, train_labels_cpu, test_embeddings_cpu, test_labels_cpu, k, metric)\n",
    "        print(f\"K={k}, Metric={metric}, Accuracy={accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task-1.2 Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_embeddings(text_embeddings, test_embeddings, test_labels, k=1, metric=\"euclidean\"):\n",
    "    correct = 0\n",
    "    total = len(test_labels)\n",
    "    \n",
    "    all_predicted = []\n",
    "    all_true = []\n",
    "    \n",
    "    for i in range(total):\n",
    "        if metric == \"euclidean\":\n",
    "            distances = np.sqrt(np.sum((text_embeddings - test_embeddings[i])**2, axis=1))\n",
    "        else:  # cosine\n",
    "            norm_text = np.linalg.norm(text_embeddings, axis=1)\n",
    "            norm_test = np.linalg.norm(test_embeddings[i])\n",
    "            distances = 1 - np.dot(text_embeddings, test_embeddings[i]) / (norm_text * norm_test)\n",
    "        \n",
    "        knn_indices = np.argsort(distances)[:k]\n",
    "        predicted_label = knn_indices[0]\n",
    "        true_label = test_labels[i]\n",
    "        \n",
    "        all_predicted.append(predicted_label)\n",
    "        all_true.append(true_label)\n",
    "        \n",
    "        correct += (predicted_label == true_label)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    \n",
    "    precision = precision_score(all_true, all_predicted, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_true, all_predicted, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_true, all_predicted, average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "distance_metrics = [\"euclidean\", \"cosine\"]\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    accuracy, precision, recall, f1 = classify_text_embeddings(\n",
    "        text_embeddings_cpu, test_embeddings_cpu, test_labels_cpu, k=1, metric=metric\n",
    "    )\n",
    "    print(f\"K=1, Metric={metric}, Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1 Score={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task-2.1 Retrieval** To check this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(indices, labels):\n",
    "    mrr_sum = 0.0\n",
    "    for i, retrieved in enumerate(indices):\n",
    "        for rank, idx in enumerate(retrieved):\n",
    "            if labels[idx] == labels[i]:\n",
    "                mrr_sum += 1.0 / (rank + 1)\n",
    "                break\n",
    "    return mrr_sum / len(indices)\n",
    "\n",
    "def precision_at_k(indices, labels, k=100):\n",
    "    precision_sum = 0.0\n",
    "    for i, retrieved in enumerate(indices):\n",
    "        relevant_retrieved = len([idx for idx in retrieved[:k] if labels[idx] == labels[i]])\n",
    "        precision_sum += relevant_retrieved / k\n",
    "    return precision_sum / len(indices)\n",
    "\n",
    "def hit_rate_at_k(indices, labels, k=100):\n",
    "    hit_rate_sum = 0.0\n",
    "    for i, retrieved in enumerate(indices):\n",
    "        if any(labels[idx] == labels[i] for idx in retrieved[:k]):\n",
    "            hit_rate_sum += 1\n",
    "    return hit_rate_sum / len(indices)\n",
    "\n",
    "def text_to_image_retrieval(train_embeddings, train_labels, text_embeddings, k=100):\n",
    "\n",
    "    nearest_neighbors_indices = []\n",
    "    for query in text_embeddings:\n",
    "        nearest_neighbors_indices.append(knn(train_embeddings, train_labels, query, k))\n",
    "    \n",
    "    mrr = mean_reciprocal_rank(nearest_neighbors_indices, train_labels)\n",
    "    precision = precision_at_k(nearest_neighbors_indices, train_labels, k)\n",
    "    hit_rate = hit_rate_at_k(nearest_neighbors_indices, train_labels, k)\n",
    "    \n",
    "    return mrr, precision, hit_rate\n",
    "\n",
    "mrr, precision, hit_rate = text_to_image_retrieval(train_embeddings_cpu, train_labels_cpu, text_embeddings_cpu, k=100)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Reciprocal Rank: {mrr}\")\n",
    "print(f\"Precision@100: {precision}\")\n",
    "print(f\"Hit Rate@100: {hit_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task-2.2 Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_image_retrieval(train_embeddings, train_labels, test_embeddings, test_labels, k=100):\n",
    "    nearest_neighbors_indices = []\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    for query in test_embeddings:\n",
    "        neighbors, comparisons = knn(train_embeddings, train_labels, query, k, return_comparisons=True)\n",
    "        nearest_neighbors_indices.append(neighbors)\n",
    "        total_comparisons += comparisons\n",
    "    \n",
    "    mrr = mean_reciprocal_rank(nearest_neighbors_indices, test_labels)\n",
    "    precision = precision_at_k(nearest_neighbors_indices, test_labels, k)\n",
    "    hit_rate = hit_rate_at_k(nearest_neighbors_indices, test_labels, k)\n",
    "    \n",
    "    avg_comparisons = total_comparisons / len(test_embeddings)\n",
    "    \n",
    "    return mrr, precision, hit_rate, avg_comparisons\n",
    "\n",
    "mrr, precision, hit_rate, avg_comparisons = image_to_image_retrieval(train_embeddings_cpu, train_labels_cpu, test_embeddings_cpu, test_labels_cpu, k=100)\n",
    "\n",
    "print(f\"Mean Reciprocal Rank: {mrr}\")\n",
    "print(f\"Precision@100: {precision}\")\n",
    "print(f\"Hit Rate@100: {hit_rate}\")\n",
    "print(f\"Average Comparisons per Query: {avg_comparisons}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LSH Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, num_bits, num_planes, dimension):\n",
    "       \n",
    "        self.num_bits = num_bits\n",
    "        self.num_planes = num_planes\n",
    "        self.dimension = dimension\n",
    "        \n",
    "        self.hyperplanes = np.random.randn(self.num_planes, self.dimension)\n",
    "        \n",
    "    def hash(self, vector):\n",
    "\n",
    "        dot_products = np.dot(self.hyperplanes, vector)\n",
    "        hash_code = (dot_products > 0).astype(int)\n",
    "        return hash_code\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        \n",
    "        self.hash_codes = np.array([self.hash(vector) for vector in dataset])\n",
    "        \n",
    "    def get_hash_buckets(self):\n",
    "        \n",
    "        hash_buckets = {}\n",
    "        for code in self.hash_codes:\n",
    "            code_tuple = tuple(code) \n",
    "            if code_tuple not in hash_buckets:\n",
    "                hash_buckets[code_tuple] = 0\n",
    "            hash_buckets[code_tuple] += 1\n",
    "        return hash_buckets\n",
    "    \n",
    "    def search(self, query_vector, k=5):\n",
    "        \n",
    "        query_hash = self.hash(query_vector)\n",
    "        distances = np.array([np.sum(query_hash != stored_hash) for stored_hash in self.hash_codes])\n",
    "        nearest_indices = np.argsort(distances)[:k]\n",
    "        return nearest_indices\n",
    "\n",
    "def compute_metrics(train_embeddings, train_labels, test_embeddings, test_labels, k=5):\n",
    "    \n",
    "    lsh = LSH(num_bits=10, num_planes=50, dimension=train_embeddings.shape[1])\n",
    "    lsh.fit(train_embeddings)\n",
    "    \n",
    "    mrr = 0\n",
    "    precision_at_k = 0\n",
    "    hit_rate = 0\n",
    "    total_queries = len(test_embeddings)\n",
    "    total_comparisons = 0 \n",
    "    \n",
    "    for query_embedding, query_label in zip(test_embeddings, test_labels):\n",
    "        retrieved_indices = lsh.search(query_embedding, k)\n",
    "        retrieved_labels = train_labels[retrieved_indices]\n",
    "        \n",
    "        rank = np.where(retrieved_labels == query_label)[0]\n",
    "        if len(rank) > 0:\n",
    "            mrr += 1 / (rank[0] + 1)\n",
    "        \n",
    "        precision_at_k += np.sum(retrieved_labels == query_label) / k\n",
    "        \n",
    "        if np.sum(retrieved_labels == query_label) > 0:\n",
    "            hit_rate += 1\n",
    "        \n",
    "        total_comparisons += len(train_embeddings)\n",
    "    \n",
    "    mrr /= total_queries\n",
    "    precision_at_k /= total_queries\n",
    "    hit_rate /= total_queries\n",
    "    \n",
    "    avg_comparisons_per_query = total_comparisons / total_queries\n",
    "    \n",
    "    return mrr, precision_at_k, hit_rate, avg_comparisons_per_query\n",
    "\n",
    "k = 100\n",
    "mrr, precision_at_k, hit_rate, avg_comparisons_per_query = compute_metrics(train_embeddings_cpu, train_labels_cpu, test_embeddings_cpu, test_labels_cpu, k)\n",
    "\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Precision@{k}: {precision_at_k:.4f}\")\n",
    "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
    "print(f\"Average Comparisons Per Query: {avg_comparisons_per_query:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IVF Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IVF:\n",
    "    def __init__(self, embeddings, n_clusters=10):\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
    "        self.kmeans.fit(embeddings)\n",
    "        \n",
    "        self.centroids = self.kmeans.cluster_centers_\n",
    "        \n",
    "        self.labels = self.kmeans.labels_\n",
    "\n",
    "    def search(self, query_embedding, nprobe=1):\n",
    "        query_cluster = self.kmeans.predict([query_embedding])[0]\n",
    "        cluster_distances = pairwise_distances_argmin_min([query_embedding], self.centroids)[1]\n",
    "        \n",
    "        closest_clusters = np.argsort(cluster_distances)[:nprobe]  # Get the top nprobe clusters\n",
    "        \n",
    "        closest_indices = []\n",
    "        for cluster in closest_clusters:\n",
    "            cluster_indices = np.where(self.labels == cluster)[0]\n",
    "            closest_indices.extend(cluster_indices)\n",
    "        \n",
    "        return closest_indices\n",
    "\n",
    "def compute_metrics(train_embeddings, train_labels, test_embeddings, test_labels, ivf, k=100, nprobe=1):\n",
    "    mrr = 0\n",
    "    precision_at_k = 0\n",
    "    hit_rate = 0\n",
    "    total_queries = len(test_labels)\n",
    "    total_comparisons = 0  \n",
    "\n",
    "    for query_embedding, query_label in zip(test_embeddings, test_labels):\n",
    "        retrieved_indices = ivf.search(query_embedding, nprobe)\n",
    "        \n",
    "        # Check if retrieved indices are valid\n",
    "        if len(retrieved_indices) == 0:\n",
    "            print(f\"No results found for query {query_label}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_labels = train_labels[retrieved_indices]\n",
    "        \n",
    "        rank = np.where(retrieved_labels == query_label)[0]\n",
    "        if len(rank) > 0:\n",
    "            mrr += 1 / (rank[0] + 1)\n",
    "        \n",
    "        precision_at_k += np.sum(retrieved_labels == query_label) / k\n",
    "        \n",
    "        if np.sum(retrieved_labels == query_label) > 0:\n",
    "            hit_rate += 1\n",
    "        \n",
    "        comparisons_for_query = 0\n",
    "        for cluster in ivf.search(query_embedding, nprobe):\n",
    "            comparisons_for_query += np.sum(ivf.labels == cluster)\n",
    "        \n",
    "        total_comparisons += comparisons_for_query\n",
    "    \n",
    "    mrr /= total_queries\n",
    "    precision_at_k /= total_queries\n",
    "    hit_rate /= total_queries\n",
    "    avg_comparisons = total_comparisons / total_queries\n",
    "    \n",
    "    return mrr, precision_at_k, hit_rate, avg_comparisons\n",
    "\n",
    "\n",
    "    \n",
    "ivf = IVF(train_embeddings, n_clusters=10)\n",
    "\n",
    "nprobe = 5 \n",
    "mrr, precision, hit_rate, avg_comparisons = compute_metrics(train_embeddings_cpu, train_labels_cpu, test_embeddings_cpu, test_labels_cpu, ivf, k=100, nprobe=nprobe)\n",
    "    \n",
    "print(f\"Mean Reciprocal Rank (MRR) @nprobe={nprobe}: {mrr:.4f}\")\n",
    "print(f\"Precision@100 @nprobe={nprobe}: {precision:.4f}\")\n",
    "print(f\"Hit Rate@100 @nprobe={nprobe}: {hit_rate:.4f}\")\n",
    "print(f\"Average Comparisons per Query @nprobe={nprobe}: {avg_comparisons:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_sizes(ivf):\n",
    "    unique_labels, counts = np.unique(ivf.labels, return_counts=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(unique_labels, counts, color='skyblue')\n",
    "    plt.title('Number of Points in Each Cluster')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of Points')\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster_sizes(ivf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparisons_vs_nprobe(train_embeddings, train_labels, test_embeddings, test_labels, ivf, nprobes_range):\n",
    "    avg_comparisons_list = []\n",
    "    \n",
    "    for nprobe in nprobes_range:\n",
    "        mrr, precision, hit_rate, avg_comparisons = compute_metrics(train_embeddings, train_labels, test_embeddings, test_labels, ivf, k=100, nprobe=nprobe)\n",
    "        avg_comparisons_list.append(avg_comparisons)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(nprobes_range, avg_comparisons_list, marker='o', color='orange')\n",
    "    plt.title('Average Number of Comparisons per Query vs nprobe')\n",
    "    plt.xlabel('nprobe')\n",
    "    plt.ylabel('Average Comparisons')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "nprobes_range = [1, 2, 3, 5, 10, 20]\n",
    "plot_comparisons_vs_nprobe(train_embeddings_cpu, train_labels_cpu, test_embeddings_cpu, test_labels_cpu, ivf, nprobes_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
